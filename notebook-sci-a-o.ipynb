{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**INTRODUCTION**\n\nA recommendation system is a type of information filtering system that predicts and suggests items or products that a user is likely to be interested in based on their past behavior, preferences, and interactions with the system.\n\nThe main goal of a recommendation system is to provide personalized recommendations to users, in order to enhance their experience, and ultimately increase user engagement and revenue for the business.\n\nThere are generally two main types of recommendation systems:\n\n> **Content-based recommendation systems** - These systems recommend items that are similar to the ones a user has already shown interest in. It analyzes the attributes or features of items and match them to user preferences.\n\n> **Collaborative filtering recommendation systems** - These systems recommend items that are popular or preferred by other users with similar preferences to the current user. It analyzes user behavior, such as ratings or purchases, and find other users with similar patterns of behavior.\n\nIt typically works by collecting data about users' behavior, preferences, and interactions with the system, such as items they have viewed, purchased, or rated. \n\n**AIM OF PROJECT**: It is develop a recommendation system that will predict the evaluation of DataLab Store products in order to recommend items most likely to be purchased by consumers.\n\n**PROCEDURE**\n\n> **Data Description:** The dataset had 1975 observations(items) with 338 features(users).\n\n> **Missing Values:** The dataset had missing values which were represented with 0 meaning no rating was done on those items.\n\nThe following steps were taken in developing the recommendation system for the project:\n> **Step 1 Using Machine Learning Algorithm to fill missing values:** The Iterative imputer was used to fill up items which had no rating this works by estimating the rating of an item by using a regression model.\n\n> **Step 2 Using Singular Value Decomposition:** This model was used to predict the final rating of the various items after training and testing on the dataset which had all values imputed now  \n","metadata":{}},{"cell_type":"code","source":"# Importation of libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn\nimport scipy\nfrom matplotlib import pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-02-21T14:34:18.426814Z","iopub.execute_input":"2023-02-21T14:34:18.427293Z","iopub.status.idle":"2023-02-21T14:34:19.046645Z","shell.execute_reply.started":"2023-02-21T14:34:18.427259Z","shell.execute_reply":"2023-02-21T14:34:19.045133Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**EXPLORATORY DATA ANALYSIS**","metadata":{}},{"cell_type":"code","source":"# Load the dataset\ndf = pd.read_csv('/kaggle/input/comptition-datalab/dataset.csv')\n#if it is in google colab after uploading the dataset\n#df = pd.read_csv(\"dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-02-21T14:34:25.790444Z","iopub.execute_input":"2023-02-21T14:34:25.790919Z","iopub.status.idle":"2023-02-21T14:34:25.931071Z","shell.execute_reply.started":"2023-02-21T14:34:25.790882Z","shell.execute_reply":"2023-02-21T14:34:25.929295Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Checking for brief data description\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-02-21T14:34:32.962218Z","iopub.execute_input":"2023-02-21T14:34:32.963591Z","iopub.status.idle":"2023-02-21T14:34:33.010815Z","shell.execute_reply.started":"2023-02-21T14:34:32.963516Z","shell.execute_reply":"2023-02-21T14:34:33.008922Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1975 entries, 0 to 1974\nColumns: 339 entries, Item to Rachid\ndtypes: float64(338), object(1)\nmemory usage: 5.1+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Checking for the data type of the features\ndf.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-02-21T14:35:09.715947Z","iopub.execute_input":"2023-02-21T14:35:09.716336Z","iopub.status.idle":"2023-02-21T14:35:09.728952Z","shell.execute_reply.started":"2023-02-21T14:35:09.716305Z","shell.execute_reply":"2023-02-21T14:35:09.727927Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Item        object\nLewis      float64\nEugene     float64\nLee        float64\nWiley      float64\n            ...   \nIsaac      float64\nLeroy      float64\nMaurice    float64\nRania      float64\nRachid     float64\nLength: 339, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"# Number of missing values\n(df == 0).sum()","metadata":{"execution":{"iopub.status.busy":"2023-02-21T14:37:35.196284Z","iopub.execute_input":"2023-02-21T14:37:35.196708Z","iopub.status.idle":"2023-02-21T14:37:35.210888Z","shell.execute_reply.started":"2023-02-21T14:37:35.196676Z","shell.execute_reply":"2023-02-21T14:37:35.209306Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Item          0\nLewis      1953\nEugene     1954\nLee        1891\nWiley      1964\n           ... \nIsaac      1929\nLeroy      1901\nMaurice    1751\nRania      1733\nRachid     1328\nLength: 339, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Number of items rated by each user\ndf.shape[0]-(df==0).sum()","metadata":{"execution":{"iopub.status.busy":"2023-02-21T14:36:58.962975Z","iopub.execute_input":"2023-02-21T14:36:58.963418Z","iopub.status.idle":"2023-02-21T14:36:58.978122Z","shell.execute_reply.started":"2023-02-21T14:36:58.963382Z","shell.execute_reply":"2023-02-21T14:36:58.977066Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Item       1975\nLewis        22\nEugene       21\nLee          84\nWiley        11\n           ... \nIsaac        46\nLeroy        74\nMaurice     224\nRania       242\nRachid      647\nLength: 339, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-02-21T14:38:44.075387Z","iopub.execute_input":"2023-02-21T14:38:44.075812Z","iopub.status.idle":"2023-02-21T14:38:44.871202Z","shell.execute_reply.started":"2023-02-21T14:38:44.075779Z","shell.execute_reply":"2023-02-21T14:38:44.869962Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"             Lewis       Eugene          Lee        Wiley       Samira  \\\ncount  1975.000000  1975.000000  1975.000000  1975.000000  1975.000000   \nmean      0.044557     0.035443     0.127342     0.021772     0.170380   \nstd       0.427096     0.365370     0.663005     0.303695     0.781925   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n75%       0.000000     0.000000     0.000000     0.000000     0.000000   \nmax       5.000000     5.000000     5.000000     5.000000     5.000000   \n\n           Jacques       Karima       George        Hakim          Tom  ...  \\\ncount  1975.000000  1975.000000  1975.000000  1975.000000  1975.000000  ...   \nmean      0.030886     0.131139     0.037468     0.050633     0.445316  ...   \nstd       0.349494     0.682698     0.383108     0.459305     1.214588  ...   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \nmax       5.000000     5.000000     4.500000     5.000000     4.500000  ...   \n\n            Arthur       Martin     Mohammed      Charles        Yusuf  \\\ncount  1975.000000  1975.000000  1975.000000  1975.000000  1975.000000   \nmean      0.392405     0.026329     0.762278     0.244051     0.132152   \nstd       1.123094     0.340325     1.276024     0.880346     0.757271   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n75%       0.000000     0.000000     2.000000     0.000000     0.000000   \nmax       5.000000     5.000000     5.000000     5.000000     5.000000   \n\n             Isaac        Leroy      Maurice        Rania       Rachid  \ncount  1975.000000  1975.000000  1975.000000  1975.000000  1975.000000  \nmean      0.071899     0.119241     0.402532     0.456203     1.189367  \nstd       0.508185     0.626043     1.154698     1.256888     1.765175  \nmin       0.000000     0.000000     0.000000     0.000000     0.000000  \n25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n75%       0.000000     0.000000     0.000000     0.000000     3.000000  \nmax       5.000000     5.000000     5.000000     5.000000     5.000000  \n\n[8 rows x 338 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Lewis</th>\n      <th>Eugene</th>\n      <th>Lee</th>\n      <th>Wiley</th>\n      <th>Samira</th>\n      <th>Jacques</th>\n      <th>Karima</th>\n      <th>George</th>\n      <th>Hakim</th>\n      <th>Tom</th>\n      <th>...</th>\n      <th>Arthur</th>\n      <th>Martin</th>\n      <th>Mohammed</th>\n      <th>Charles</th>\n      <th>Yusuf</th>\n      <th>Isaac</th>\n      <th>Leroy</th>\n      <th>Maurice</th>\n      <th>Rania</th>\n      <th>Rachid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>...</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n      <td>1975.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.044557</td>\n      <td>0.035443</td>\n      <td>0.127342</td>\n      <td>0.021772</td>\n      <td>0.170380</td>\n      <td>0.030886</td>\n      <td>0.131139</td>\n      <td>0.037468</td>\n      <td>0.050633</td>\n      <td>0.445316</td>\n      <td>...</td>\n      <td>0.392405</td>\n      <td>0.026329</td>\n      <td>0.762278</td>\n      <td>0.244051</td>\n      <td>0.132152</td>\n      <td>0.071899</td>\n      <td>0.119241</td>\n      <td>0.402532</td>\n      <td>0.456203</td>\n      <td>1.189367</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.427096</td>\n      <td>0.365370</td>\n      <td>0.663005</td>\n      <td>0.303695</td>\n      <td>0.781925</td>\n      <td>0.349494</td>\n      <td>0.682698</td>\n      <td>0.383108</td>\n      <td>0.459305</td>\n      <td>1.214588</td>\n      <td>...</td>\n      <td>1.123094</td>\n      <td>0.340325</td>\n      <td>1.276024</td>\n      <td>0.880346</td>\n      <td>0.757271</td>\n      <td>0.508185</td>\n      <td>0.626043</td>\n      <td>1.154698</td>\n      <td>1.256888</td>\n      <td>1.765175</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>4.500000</td>\n      <td>5.000000</td>\n      <td>4.500000</td>\n      <td>...</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 338 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Preprocessing**","metadata":{}},{"cell_type":"code","source":"from keras.layers import Input, Dense\nfrom keras.models import Model\n\n\ndf2 = df.copy()\n\n# Replace missing values with 0\ndf2.replace(0, np.nan, inplace=True)\n\n# Split the dataset into input (X) and output (y)\nX = df2.iloc[:, 1:].values\ny = df2.iloc[:, 0].values\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-18T14:11:29.005391Z","iopub.execute_input":"2023-02-18T14:11:29.005905Z","iopub.status.idle":"2023-02-18T14:17:12.807503Z","shell.execute_reply.started":"2023-02-18T14:11:29.005857Z","shell.execute_reply":"2023-02-18T14:17:12.805394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first procedure taken for this work was to fill missing values using various machine learning techniques namely:\n\n> **Simple Imputer**: This technic works by replacing missing values with a specified statistic, such as the mean, median, or mode in the same column.\n\n> **KNN Imputer**: This technic works by finding the K closest observations to the observation with the missing value(s), and using the values from those K neighbors to impute the missing value.\n\n> **Iterative Imputer**: This technic works by using an iterative algorithm that uses regression models to estimate the missing values.\n\nAfter exploring all techniques listed, **the Iterative imputer gave a better result** which was eventually adopted for filling the missing values in the first step.","metadata":{}},{"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=30, random_state=0)\nX = imp.fit_transform(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3 = df2.copy()\n# Replace back into a copy of the original dataset\ndf3.iloc[:, 1:] = X","metadata":{"execution":{"iopub.status.busy":"2023-02-18T14:17:12.820409Z","iopub.execute_input":"2023-02-18T14:17:12.822347Z","iopub.status.idle":"2023-02-18T14:17:13.051223Z","shell.execute_reply.started":"2023-02-18T14:17:12.822242Z","shell.execute_reply":"2023-02-18T14:17:13.049307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshape the data from wide to long format using the pivot_table method\ndf_long = df3.melt(id_vars=\"Item\", var_name=\"customer_name\", value_name=\"rating\")\n\n\n# Verify the shape of the long format data\nprint(\"Number of rows:\", df_long.shape[0])\nprint(\"Number of columns:\", df_long.shape[1])","metadata":{"execution":{"iopub.status.busy":"2023-02-18T14:17:13.055550Z","iopub.execute_input":"2023-02-18T14:17:13.056331Z","iopub.status.idle":"2023-02-18T14:17:13.221793Z","shell.execute_reply.started":"2023-02-18T14:17:13.056237Z","shell.execute_reply":"2023-02-18T14:17:13.220260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Subseting to get the right dataset\ndf4 = df_long.copy()\n\n#Setting new index\ndf4['Index'] = range(len(df4))\ndf4 = df4.set_index('Index')\n\ndf4.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-18T14:17:13.223235Z","iopub.execute_input":"2023-02-18T14:17:13.223756Z","iopub.status.idle":"2023-02-18T14:17:13.348861Z","shell.execute_reply.started":"2023-02-18T14:17:13.223708Z","shell.execute_reply":"2023-02-18T14:17:13.347100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Our dataset is ready for use !!!\ndf4.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T14:17:13.350899Z","iopub.execute_input":"2023-02-18T14:17:13.351507Z","iopub.status.idle":"2023-02-18T14:17:13.375217Z","shell.execute_reply.started":"2023-02-18T14:17:13.351437Z","shell.execute_reply":"2023-02-18T14:17:13.373779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Modeling : Singular Value Decomposition(SVD)**","metadata":{}},{"cell_type":"markdown","source":"There are several models and techniques that can be used to predict missing values in a matrix (Matrix Completion using SVD, Matrix Factorization, K-Nearest Neighbors (KNN), Deep Learning Models).\nThe most accurate, based on our data is SVD","metadata":{}},{"cell_type":"markdown","source":"Singular Value Decomposition is a mathematical technique that has many applications in data analysis, such as in principal component analysis, image compression, and text processing.\n\nOne of the applications of SVD is in matrix completion, where the goal is to predict the missing values of a partially observed matrix. Given a partially observed matrix A, SVD can be used to predict the missing values by approximating the original matrix with a low-rank matrix.\n\n","metadata":{}},{"cell_type":"markdown","source":"To implement this, we will use the library 'surprise'","metadata":{}},{"cell_type":"code","source":"pip install surprise","metadata":{"execution":{"iopub.status.busy":"2023-02-18T14:17:13.380111Z","iopub.execute_input":"2023-02-18T14:17:13.380580Z","iopub.status.idle":"2023-02-18T14:17:29.022681Z","shell.execute_reply.started":"2023-02-18T14:17:13.380538Z","shell.execute_reply":"2023-02-18T14:17:29.021459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Modeling\n#SVD APPROACH\nfrom surprise import Reader, Dataset\nfrom surprise.model_selection import train_test_split\nfrom surprise import SVD\nfrom surprise import accuracy\n\n# Load the user-item rating data into a Pandas DataFrame\n\n# Convert the DataFrame into a surprise dataset\nreader = Reader(rating_scale=(0.5, 5))\ndata = Dataset.load_from_df(df4, reader)\n\n# Split the dataset into training and testing sets\ntrain_set, test_set = train_test_split(data, test_size=0.3, random_state=5)\n\n# Train the SVD model\nalgo = SVD()\nalgo.fit(train_set)\n\n# Make predictions for the test set\npredictions = algo.test(test_set)\n\n# Evaluate the accuracy of the predictions\naccuracy.mae(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T14:17:29.024155Z","iopub.execute_input":"2023-02-18T14:17:29.024606Z","iopub.status.idle":"2023-02-18T14:18:07.142443Z","shell.execute_reply.started":"2023-02-18T14:17:29.024556Z","shell.execute_reply":"2023-02-18T14:18:07.141177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predictions to submit for the competition**","metadata":{}},{"cell_type":"code","source":"# Upload the test data we are supposed to make prediction on\nsub_test = pd.read_csv('/kaggle/input/comptition-datalab/test.csv')\n\nsub_test.shape\n#All the items are in the test dataset","metadata":{"execution":{"iopub.status.busy":"2023-02-18T14:18:07.369628Z","iopub.execute_input":"2023-02-18T14:18:07.370026Z","iopub.status.idle":"2023-02-18T14:18:07.385554Z","shell.execute_reply.started":"2023-02-18T14:18:07.369994Z","shell.execute_reply":"2023-02-18T14:18:07.384184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T14:18:07.387840Z","iopub.execute_input":"2023-02-18T14:18:07.388404Z","iopub.status.idle":"2023-02-18T14:18:07.402385Z","shell.execute_reply.started":"2023-02-18T14:18:07.388353Z","shell.execute_reply":"2023-02-18T14:18:07.401118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding a column of Ratings we got with our Imputer\n# This is not yet the predictions\nsub_test['Rating'] = -1.000000\nfor i in range(len(sub_test)) :\n  sub_test['Rating'][i]= (df4['rating'][(df4['Item'] == sub_test['Item'][i])&(df4['customer_name']== sub_test['User'][i])]).values[0]\n\nsub_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T14:18:59.478936Z","iopub.execute_input":"2023-02-18T14:18:59.479577Z","iopub.status.idle":"2023-02-18T14:21:55.760812Z","shell.execute_reply.started":"2023-02-18T14:18:59.479529Z","shell.execute_reply":"2023-02-18T14:21:55.759346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform the prediction dataset into a format we can use for prediction\ndf_tuples = [tuple(row) for row in sub_test.values]# Make predictions for the test set\n# Make predictions for the test set\nsub_pred = algo.test(df_tuples)\n# Evaluate the accuracy of the predictions\naccuracy.mae(sub_pred)","metadata":{"execution":{"iopub.status.busy":"2023-02-18T14:22:29.416052Z","iopub.execute_input":"2023-02-18T14:22:29.416531Z","iopub.status.idle":"2023-02-18T14:22:29.453597Z","shell.execute_reply.started":"2023-02-18T14:22:29.416493Z","shell.execute_reply":"2023-02-18T14:22:29.452084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Description of the predictions\npredic = pd.DataFrame(sub_pred)\npredic.describe()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T14:22:42.960192Z","iopub.execute_input":"2023-02-18T14:22:42.960804Z","iopub.status.idle":"2023-02-18T14:22:42.985586Z","shell.execute_reply.started":"2023-02-18T14:22:42.960760Z","shell.execute_reply":"2023-02-18T14:22:42.984207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare our submission dataset\nexp = predic.copy()\nexp['Rating'] = exp['est']\nexp.drop(columns = ['uid', 'iid', 'r_ui', 'details', 'est'], inplace = True)\nexp.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-18T14:23:00.898974Z","iopub.execute_input":"2023-02-18T14:23:00.899531Z","iopub.status.idle":"2023-02-18T14:23:00.917938Z","shell.execute_reply.started":"2023-02-18T14:23:00.899480Z","shell.execute_reply":"2023-02-18T14:23:00.916287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to export the prediction data and download it\n#from google.colab import files\n#exp.to_csv('prediction.csv', encoding = 'utf-8-sig') \n#files.download('prediction.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we go !!!\nThank you !","metadata":{}}]}